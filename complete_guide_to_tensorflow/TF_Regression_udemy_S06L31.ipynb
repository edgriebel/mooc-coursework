{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6 Lecture 31: TensorFlow Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want a big data set this time\n",
    "x_data = np.linspace(0.0,10.0,1000000)\n",
    "noise = np.random.randn(len(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noise\n",
    "#x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = mx + b$\n",
    "\n",
    "$b = 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = (0.5 * x_data) + 5 + noise # add some noise so it's not a perfect line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pandas to make charts and other ops easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = pd.DataFrame(data=x_data,columns=['X Data'])\n",
    "y_df = pd.DataFrame(data=y_true,columns=['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now concatenate 2 data frames\n",
    "my_data = pd.concat([x_df, y_df], axis=1) # join them on columns rather than as \"pancakes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_data.plot() # too much data, we may crash factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.sample(n=250).plot(kind='scatter',x='X Data',y='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^^^ linear trend but not straight because we've added noise\n",
    "# can we get TF to find a line here? We know intercept should be ~5 \n",
    "# and slope should be 0.5\n",
    "# But we can't feed 1mm points to TF, we have to batch it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to using TF:\n",
    "* Define constants\n",
    "* Define variables\n",
    "* Define placeholders\n",
    "* Define model\n",
    "* define graph\n",
    "* define loss function\n",
    "* define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8 # grab 8 pts at a time, choosing it is an art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randn(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.Variable(-0.00361972)\n",
    "b = tf.Variable(-1.4707748)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xph = tf.placeholder(tf.float32,[batch_size])\n",
    "yph = tf.placeholder(tf.float32,[batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model = m*xph + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "# we could use **2 but use tf function for consistency with documentation\n",
    "error = tf.reduce_sum(tf.square(yph-y_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define optimizer and training\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test passing an array (rather than a scalar) as an index\n",
    "multi_index = range(0,100,7)\n",
    "x_data[multi_index]\n",
    "# it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # we need to initialize locals as well as globals [why??]\n",
    "    # run \"some\" batches\n",
    "    # 8,000 samples _should_ be enough\n",
    "    batches = 1000\n",
    "    for i in range(batches):\n",
    "        # rand_ind is fetch random indexes of the data\n",
    "        rand_ind = np.random.randint(len(x_data), size=batch_size)\n",
    "        # fill the random index points into the TF graph\n",
    "        feed_dict = {xph:x_data[rand_ind], yph:y_true[rand_ind]}\n",
    "        # Now train with these random points\n",
    "        sess.run(train,feed_dict)\n",
    "        model_m, model_b = sess.run([m,b])\n",
    "        val.append((model_m,model_b,i))\n",
    "    \n",
    "\n",
    "    print(f'len {len(val)} 0={val[0]} last={val[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax=fig.add_subplot(111,projection='3d')\n",
    "val_x,val_y,val_z = list(zip(*val))\n",
    "plt.plot(val_x,val_y,val_z,marker='.')\n",
    "plt.scatter(val_x[-1],val_y[-1],val_z[-1],c='r', marker='+')\n",
    "print(val_x[-1],val_y[-1],len(val[:10]))\n",
    "ax.set_xlabel('slope')\n",
    "ax.set_ylabel('intercept')\n",
    "ax.set_zlabel('time')\n",
    "ax.set_xlim(min(val_x),max(val_x))\n",
    "ax.set_ylim(min(val_y),max(val_y))\n",
    "ax.set_zlim(min(val_z),max(val_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We started with orig slope of 0.5 then preturbed it with noise so m should be close to 0.5\n",
    "# Likewise, b started close to 5, so it should end up being close to 5\n",
    "print(f'm={model_m} b={model_b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we visualize\n",
    "y_hat = x_data * model_m + model_b\n",
    "my_data.sample(250).plot(kind='scatter',x='X Data', y='Y')\n",
    "my_val_samples = np.array(val)[np.linspace(0,len(val)-1,10,dtype=int)]#.sample(10)\n",
    "for points in my_val_samples:\n",
    "    _y_hat = x_data * points[0] + points[1]\n",
    "    _col = 1-(points[2] / len(val))\n",
    "    plt.plot(x_data, _y_hat, color=(_col, .8, .8))\n",
    "plt.plot(x_data, y_hat, 'r')\n",
    "plt.ylim(0,my_data.Y.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Session 6, Lecture 32 Using Estimator API (and Train/Test Fit)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use TF but there are others like Keras and Layers, look at them at the end of course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have a single feature column that happens to be a numeric\n",
    "feat_cols = [tf.feature_column.numeric_column('x',shape=[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now set up estimator, ignore errors\n",
    "# similar to a Scikit-learn estimator\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=feat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_eval, y_train, y_eval = train_test_split(x_data, y_true, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape) # 70% of 1 mil == 700k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_eval.shape # same, 30% of 1mil == 300k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input fucntion that acts like feed dictionary and batch size indicator all at once\n",
    "# It can take in numpy *and* pandas\n",
    "\n",
    "# \"x\" is dictionary, so pass in same as the feat_cols\n",
    "input_func = tf.estimator.inputs.numpy_input_fn({'x': x_train}, y_train,  # train data\n",
    "                                                batch_size=8, num_epochs=None, shuffle=True) # other args for batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_func = tf.estimator.inputs.numpy_input_fn({'x': x_train}, y_train,  # train data\n",
    "                                                batch_size=8, num_epochs=1000, shuffle=False) # shuffle = false because we're going to use this for eval against a test func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_func = tf.estimator.inputs.numpy_input_fn({'x': x_eval}, y_eval,  # train data\n",
    "                                                batch_size=8, num_epochs=1000, shuffle=False) # shuffle = false because we're going to use this for eval against a test func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=input_func, steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets get estimator metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to values\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_func, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = estimator.evaluate(input_fn=eval_input_func,steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TRAINING DATA METRICS')\n",
    "print(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now compare to test set metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('EVAL METRICS')\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for overfit by comparing train loss to test loss. if test loss is a lot bigger then it's overfit\n",
    "You should expect that test loss is higher but should be close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So how do we predict new values??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_new_data = np.linspace(0,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn_predict = tf.estimator.inputs.numpy_input_fn({'x': brand_new_data}, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.predict(input_fn=input_fn_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_note output is a generator object, so we can cast to a list or iterate thru values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = []\n",
    "#for prod in estimator.predict(input_fn=input_fn_predict):\n",
    "#    predictions.append(pred['predictions'])\n",
    "predictions = [pred['predictions'] for pred in estimator.predict(input_fn=input_fn_predict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now plot and see how we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.sample(n=250).plot(kind='scatter', x='X Data', y='Y')\n",
    "plt.plot(brand_new_data,predictions,'r*')\n",
    "# Note how close the plotted predicted points are to the existing data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
